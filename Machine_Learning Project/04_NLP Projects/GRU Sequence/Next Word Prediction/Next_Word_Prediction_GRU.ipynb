{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Love1117/Machine_learning-Projects/blob/main/Machine_Learning%20Project/04_NLP%20Projects/GRU%20Sequence/Next%20Word%20Prediction/Next_Word_Prediction_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project Summary: Next-Word Prediction Using GRU-Based Language Model**\n",
        "\n",
        "##**Overview**\n",
        "\n",
        "This project develops a Recurrent Neural Network (RNN) using Gated Recurrent Units (GRU) to perform next-word prediction on the classic novel \"War and Peace\" sourced from Project Gutenberg. After thorough preprocessing and tokenization (without applying n-grams or padding), the model was trained to learn long-range linguistic patterns.\n",
        "The GRU architecture delivered 79% prediction accuracy, outperforming a previous LSTM implementation in both speed and predictive consistency.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##**Aim of the Project**\n",
        "\n",
        "To build a lightweight and effective language model capable of understanding literary text structure.\n",
        "\n",
        "To compare GRU performance against LSTM for next-word prediction tasks.\n",
        "\n",
        "To explore how GRU networks handle long text dependencies without additional sequence engineering like n-gram creation or padding.\n",
        "\n"
      ],
      "metadata": {
        "id": "ljPdXIgAyq4m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxTzbWKtYRYW"
      },
      "source": [
        "##**IMPORTING NECCESARY LIBERTIES FOR MY PROJECT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fc_DEOMj3SL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f040d09-1aeb-4201-c75c-2388f45394ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense,GRU, Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-7qztrxYUTD"
      },
      "source": [
        "##**LOADING FILE FROM GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qliLvHNF3SUM"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/My Drive/Text Data/War and Peace.txt\"\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "k82SKFdV3Scx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "c10ca232-fad4-46d9-cb6b-b0b39b709f7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The Project Gutenberg eBook of War and Peace\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it  give it away or re use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States \\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.\\n\\nTitle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "text[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5JmT1m3Yf75"
      },
      "source": [
        "##**CLEAN AND TOKENIZE TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5F9sNdYiYfaf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XH9Xk2oF3Sj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "966701ff-b216-477e-e2db-9636e3c7bbeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg eBook of War and Peace This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it give it away or re use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States you will have to check the laws of the country where you are located before using this eBook. Title War and '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import re\n",
        "text = re.sub(r'[^a-zA-Z0-9.\\s]', ' ', text)\n",
        "clean_text = re.sub(r\"\\s+\", \" \", text)\n",
        "clean_text = clean_text.strip()\n",
        "clean_text[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QU434C7I3SsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4f4568-075a-4c8c-d510-537ef764ea4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14428"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([clean_text])\n",
        "vocal_size = len(tokenizer.word_index)+1\n",
        "vocal_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuJwodSVYm5Z"
      },
      "source": [
        "##**GIVING NUMERIC REPRESENTATION FOR MY TEXT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mn0Mv40o3S1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44e032f-2a7e-40e5-ae80-146cfe21aa24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of sequence: 372367\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2492, 4950, 3957],\n",
              " [2492, 4950, 3957, 4],\n",
              " [4950, 3957, 4, 241],\n",
              " [3957, 4, 241, 2],\n",
              " [4, 241, 2, 510],\n",
              " [241, 2, 510, 38],\n",
              " [2, 510, 38, 3957],\n",
              " [510, 38, 3957, 27],\n",
              " [38, 3957, 27, 25],\n",
              " [3957, 27, 25, 1],\n",
              " [27, 25, 1, 796],\n",
              " [25, 1, 796, 4],\n",
              " [1, 796, 4, 305],\n",
              " [796, 4, 305, 2353],\n",
              " [4, 305, 2353, 7],\n",
              " [305, 2353, 7, 1],\n",
              " [2353, 7, 1, 2493],\n",
              " [7, 1, 2493, 4951],\n",
              " [1, 2493, 4951, 2],\n",
              " [2493, 4951, 2, 272]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "sequence = tokenizer.texts_to_sequences([clean_text])[0]\n",
        "input_sequence =[]\n",
        "for i in range(3,len(sequence)):\n",
        "  ngram = sequence[i-3:i+1]\n",
        "  input_sequence.append(ngram)\n",
        "print(f\"Length of sequence: {len(sequence)}\")\n",
        "input_sequence[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzTYo36WZvr2"
      },
      "source": [
        "##**CREATE TRAINING DATA:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TErNXQmv3S9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa350404-aa7d-4466-8a42-8430c7e02195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   1 2492 4950]\n",
            " [2492 4950 3957]\n",
            " [4950 3957    4]\n",
            " ...\n",
            " [ 113  250 4243]\n",
            " [ 250 4243  209]\n",
            " [4243  209   23]]\n",
            "[3957    4  241 ...  209   23   41]\n"
          ]
        }
      ],
      "source": [
        "input_sequence = np.array(input_sequence)\n",
        "\n",
        "x = input_sequence[:,:-1]\n",
        "y = input_sequence[:,-1]\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPzjUm9RZ4Nl"
      },
      "source": [
        "##**BUILDING MY GRU MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z33-nkhB3TDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f6e521-f976-450c-8f59-2be3eb92f213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.0651 - loss: 7.1640\n",
            "Epoch 2/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 49ms/step - accuracy: 0.1469 - loss: 5.5582\n",
            "Epoch 3/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 49ms/step - accuracy: 0.1784 - loss: 5.0462\n",
            "Epoch 4/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 50ms/step - accuracy: 0.2012 - loss: 4.6504\n",
            "Epoch 5/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.2236 - loss: 4.2748\n",
            "Epoch 6/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 50ms/step - accuracy: 0.2548 - loss: 3.9126\n",
            "Epoch 7/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 50ms/step - accuracy: 0.2923 - loss: 3.5797\n",
            "Epoch 8/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.3330 - loss: 3.2786\n",
            "Epoch 9/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.3683 - loss: 3.0201\n",
            "Epoch 10/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.4052 - loss: 2.7828\n",
            "Epoch 11/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.4399 - loss: 2.5754\n",
            "Epoch 12/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.4747 - loss: 2.3837\n",
            "Epoch 13/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 51ms/step - accuracy: 0.5039 - loss: 2.2159\n",
            "Epoch 14/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.5322 - loss: 2.0680\n",
            "Epoch 15/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.5574 - loss: 1.9326\n",
            "Epoch 16/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.5798 - loss: 1.8127\n",
            "Epoch 17/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6011 - loss: 1.7060\n",
            "Epoch 18/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6211 - loss: 1.6036\n",
            "Epoch 19/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.6388 - loss: 1.5167\n",
            "Epoch 20/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6551 - loss: 1.4407\n",
            "Epoch 21/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6699 - loss: 1.3664\n",
            "Epoch 22/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.6822 - loss: 1.3018\n",
            "Epoch 23/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.6953 - loss: 1.2412\n",
            "Epoch 24/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7055 - loss: 1.1851\n",
            "Epoch 25/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7162 - loss: 1.1354\n",
            "Epoch 26/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7237 - loss: 1.0989\n",
            "Epoch 27/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.7289 - loss: 1.0633\n",
            "Epoch 28/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.7358 - loss: 1.0296\n",
            "Epoch 29/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.7431 - loss: 0.9970\n",
            "Epoch 30/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7463 - loss: 0.9722\n",
            "Epoch 31/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7516 - loss: 0.9449\n",
            "Epoch 32/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7566 - loss: 0.9233\n",
            "Epoch 33/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7598 - loss: 0.9006\n",
            "Epoch 34/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7623 - loss: 0.8832\n",
            "Epoch 35/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.7648 - loss: 0.8730\n",
            "Epoch 36/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.7668 - loss: 0.8511\n",
            "Epoch 37/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - accuracy: 0.7713 - loss: 0.8335\n",
            "Epoch 38/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 53ms/step - accuracy: 0.7721 - loss: 0.8283\n",
            "Epoch 39/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.7736 - loss: 0.8168\n",
            "Epoch 40/40\n",
            "\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.7754 - loss: 0.8021\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ad5150393d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model = Sequential([Embedding(vocal_size, 500, input_length = 3),\n",
        "                    GRU(500),\n",
        "                    Dense(vocal_size, activation=\"softmax\")])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x, y, epochs=40,batch_size=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qssfZDGXZ-Rs"
      },
      "source": [
        "##**MODEL PERFORMANCE| SCORE 88%**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xpkT28Z23TKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4502a533-6957-487f-c330-fe29a9faec1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m11637/11637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 5ms/step - accuracy: 0.8023 - loss: 0.6978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7021440863609314, 0.8010038733482361]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.evaluate(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TVF03ECz3TPQ"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/My Drive/Text Data/GRU_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(tokenizer, \"/content/drive/My Drive/Text Data/GRU_tokenizer.plk\")\n"
      ],
      "metadata": {
        "id": "j1o-K3VHhHtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fb34e6-65c7-42a4-d5c6-f51f1360452b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Text Data/GRU_tokenizer.plk']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvqL7GjuaAjo"
      },
      "source": [
        "##**LOADING MY SAVE MODEL AND TOKENIZER**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "my_model = load_model(\"/content/drive/My Drive/Text Data/GRU_model.keras\")"
      ],
      "metadata": {
        "id": "Oe4zLSuugn_P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R74EQoUGaD96"
      },
      "source": [
        "##**PREDICTION — Next 4 words from 3 input words**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Oo3UXpHm3TUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3658532-a3ad-4265-9fa0-380074277d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you may copy it give it to\n"
          ]
        }
      ],
      "source": [
        "def prediction(input_word, len_of_words, tokenizer, model): # 'model' parameter is unused, 'my_model' is used from global scope.\n",
        "  input_text = input_word\n",
        "  num_words = len_of_words\n",
        "\n",
        "  generated_text = input_word # Initialize the text that will be generated\n",
        "\n",
        "  for _ in range(num_words):\n",
        "    seq = tokenizer.texts_to_sequences([generated_text])[0] # Use generated_text for context\n",
        "\n",
        "    if len(seq) < 3:\n",
        "      print(\"Input words not in vocabulary or sequence is too short. Stopping prediction.\")\n",
        "      break\n",
        "\n",
        "    seq_to_predict = seq[-3:]\n",
        "\n",
        "    seq_to_predict = np.array(seq_to_predict).reshape(1, 3)\n",
        "\n",
        "\n",
        "    pred_index = np.argmax(model.predict(seq_to_predict, verbose=0), axis=1)[0]\n",
        "\n",
        "    next_word = \"\"\n",
        "    next_word_found = False\n",
        "\n",
        "    for win, idx in tokenizer.word_index.items():\n",
        "      if idx == pred_index:\n",
        "        next_word = win\n",
        "        next_word_found = True\n",
        "        break\n",
        "\n",
        "    if next_word_found:\n",
        "      generated_text += \" \" + next_word\n",
        "    else:\n",
        "      print(f\"Predicted index {pred_index} not found in tokenizer vocabulary. Stopping prediction.\")\n",
        "      break\n",
        "\n",
        "  return generated_text\n",
        "\n",
        "print(prediction(input_word= \"you may copy\", len_of_words=4, tokenizer=tokenizer, model=model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR_w_wJzaOfJ"
      },
      "source": [
        "##**PREDICTING THE NEXT 50 WORDS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7q8iAwvh3TaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44162ea-ff4a-48ce-8450-78e1c666dafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you may copy it give it to the committee i do not think so think what about asked prince andrew with a characteristic desire to foment his own grief decided that he must retreat as quickly as possible and flying away like this take care you ll fall out he heard the voice\n"
          ]
        }
      ],
      "source": [
        "print(prediction(input_word= \"you may copy\", len_of_words=50, tokenizer=tokenizer, model=model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FYpU88cf3UwG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Conclusion / Deployment Summary**\n",
        "\n",
        "When deployed, this model can:\n",
        "\n",
        "Generate coherent next-word predictions for text composition and auto-completion tasks.\n",
        "\n",
        "Support creative writing aids, typing assistants, or text generation pipelines.\n",
        "\n",
        "Enhance NLP applications that require fast, context-aware suggestions.\n",
        "\n",
        "\n",
        "Its strong performance on a complex literary dataset demonstrates its reliability for real-world next-word prediction scenarios."
      ],
      "metadata": {
        "id": "EWs2DDHv6TyB"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMl0sVior7CYsk+0DkWyZqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}